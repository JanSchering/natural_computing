{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Implement the PSO algorithm for clustering described in “Van der Merwe, D.\n",
    "W., and Andries Petrus Engelbrecht. ”Data clustering using particle swarm\n",
    "optimization.” Evolutionary Computation, 2003. CEC’03. The 2003 Congress\n",
    "on. Vol. 1. IEEE, 2003.” (see also the lecture’s slides on swarm intelligence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to initialize population\n",
    "def init_population(size:int, K:int, data_dim:int) -> np.ndarray:\n",
    "    \"\"\"Generates a population of particles as a Matrix of shape (num_particles, K, data_dim). \n",
    "    A particle is a Matrix of length <K> where each column represents a centroid. The amount of rows\n",
    "    is equal to the dimensionality of the data that we want to cluster\n",
    "\n",
    "    Args:\n",
    "        size (int): The amount of particles \n",
    "        K (int): The dimensionality of each particle\n",
    "        data_dim (int): Dimensionality of the Data to cluster\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray): The population in Matrix form\n",
    "    \"\"\"\n",
    "    # Generate a population Matrix\n",
    "    return np.random.uniform(-1,1,(size, K, data_dim))\n",
    "\n",
    "# Sanity check to ensure that we get the right dimensionality\n",
    "assert init_population(4,4,4).shape == (4,4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d(x,y) = \\sqrt{\\sum_{k=1}^{N_d} (x_k - y_k)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.121320343559643"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_distance(x:np.ndarray, y:np.ndarray) -> float:\n",
    "    \"\"\"Calculates the euclidean distance between point <x> and point <y>.\n",
    "\n",
    "    Args:\n",
    "        p1 (np.ndarray): point1\n",
    "        p2 (np.ndarray): point2\n",
    "\n",
    "    Returns:\n",
    "        float: The distance between the points\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "# Sanity check\n",
    "assert calc_distance(np.array([5,5]), np.array([6,6])) == 1.4142135623730951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Fitness Function\n",
    "def calc_fitness(assignments: np.ndarray, dataset: np.ndarray, particle:np.ndarray) -> float:\n",
    "    \"\"\"Calculates the fitness of a particle given a K-cluster problem setup. Each entry in the particle is\n",
    "    treated as a centroid for a cluster. Calculates the normalized\n",
    "    sum of euclidean distances between centroids and assigned datapoints. \n",
    "\n",
    "    Args:\n",
    "        assignments (np.ndarray): Stores the cluster index for each datapoint\n",
    "        dataset (np.ndarray): The datapoints \n",
    "        particle (np.ndarray): The centroid Matrix\n",
    "\n",
    "    Returns:\n",
    "        float: Fitness of the particle\n",
    "    \"\"\"\n",
    "    _, num_clusters = particle.shape\n",
    "    fitness = 0\n",
    "    for cluster in range(num_clusters):\n",
    "        # Get the centroid of the cluster \n",
    "        centroid = particle[:,cluster]\n",
    "        # Get all datapoints that were assigned to this cluster\n",
    "        boolean_mask = assignments==cluster\n",
    "        datapoints = dataset[boolean_mask]\n",
    "        # Calculate the sum of distances between the datapoints and the centroid\n",
    "        total_distance = 0\n",
    "        for datapoint in datapoints:\n",
    "            total_distance += calc_distance(centroid, datapoint)\n",
    "        #normalize by the amount of assigned datapoints and add to the fitness\n",
    "        fitness +=  total_distance / len(assignments[boolean_mask])\n",
    "    # Normalize by the amount of clusters\n",
    "    fitness /= num_clusters\n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the fitness function\n",
    "\n",
    "# single datapoint\n",
    "test_ass = np.array([0])\n",
    "test_data = np.zeros([1,2])+5\n",
    "# one cluster\n",
    "test_part = np.zeros([2,1])+6\n",
    "\n",
    "assert calc_fitness(test_ass, test_data, test_part) == 1.4142135623730951\n",
    "\n",
    "# two datapoints with single cluster\n",
    "test_ass = np.array([0, 0])\n",
    "test_data = np.zeros([2,2])\n",
    "test_data[0] += 4\n",
    "test_data[1] += 5\n",
    "\n",
    "correct = (calc_distance(np.array([4,4]), np.array([6,6])) + calc_distance(np.array([5,5]), np.array([6,6]))) / 2\n",
    "assert calc_fitness(test_ass, test_data, test_part) == correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_i := \\omega\\times v_i + \\alpha_1\\times r_1\\circ (\\hat{x} - x_i) + \\alpha_2\\times r_2\\circ (\\hat{g} - x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to update the velocity of a particle\n",
    "def update_velocity(vel, local_best, global_best, current, omega, alpha1, alpha2, r1, r2) -> np.ndarray:\n",
    "    \"\"\"Updates the velocity <vel> of the <current> particle according to the PSO velocity update\n",
    "\n",
    "    Args:\n",
    "        vel (np.ndarray): Defines the velocity of the particle\n",
    "        local_best (np.ndarray): Stores the fittest version of this particle found so far\n",
    "        global_best (np.ndarray): Stores the fittest particle of the population found so far\n",
    "        current (np.ndarray): The current particle\n",
    "        omega (float): Inertia weight\n",
    "        alpha1 (float): weight param for the local influence\n",
    "        alpha2 (float): weight param for the social influence\n",
    "        r1 (float): additional weight param for local influence\n",
    "        r2 (float): additional weight param for social influence\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The updated velocity\n",
    "    \"\"\"\n",
    "    return omega*vel + alpha1*r1*(local_best - current) + alpha2*r2*(global_best - current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_i := x_i + v_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function for the position update\n",
    "def update_position(current, velocity) -> np.ndarray:\n",
    "    \"\"\"Updates the position <current>, using the <velocity> according to the PSO position update rule\n",
    "    Args:\n",
    "        current (np.ndarray): The current particle\n",
    "        velocity (_type_): The velocity of the particle\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The updated particle position\n",
    "    \"\"\"\n",
    "    new_pos = current + velocity\n",
    "    return new_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "  <li>Initialize each particle to contain N randomly selected centroids</li>\n",
    "  <li>\n",
    "    For iter=1 to maxiter:\n",
    "    <ul>\n",
    "      <li> \n",
    "        For each particle x:\n",
    "        <ul>\n",
    "          <li> For each data point z:</li>\n",
    "          <ul>\n",
    "            <li> Calculate the distance of z to each cluster centroid </li>\n",
    "            <li> Assign z to the cluster (centroid) with minimal distance; </li>\n",
    "          </ul>\n",
    "          <li> Compute the fitness (e.g. average weighted distance of data points \n",
    "            from their centroid); </li>\n",
    "          <li> Update the local best; </li>\n",
    "        </ul>\n",
    "        <li> Update the global best; </li>\n",
    "        <li> Update each particle !\"using PSO update rules </li>\n",
    "      </li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that runs the K-Means PSO algorithm\n",
    "def k_means_PSO(K:int, data_dim: int, dataset:np.ndarray, pop_size:int, max_iter:int, omega:float, alpha1:float, alpha2:float, r1:float, r2:float) -> np.ndarray:\n",
    "    \"\"\"Runs a K-Means PSO algorithm for <max_iter> iterations to find a set of <K> clusters for a dataset.\n",
    "\n",
    "    Args:\n",
    "        K (int): The amount of clusters to separate the data into\n",
    "        data_dim (int): The dimensionality of the data\n",
    "        dataset (np.ndarray): The datapoints that should be clustered\n",
    "        pop_size (int): The amount of particles to run the PSO algorithm with\n",
    "        max_iter (int): The maximum amount of iterations to run PSO for\n",
    "        omega (float): Inertia weight\n",
    "        alpha1 (float): local influence param\n",
    "        alpha2 (float): social influence param\n",
    "        r1 (float): additional local influence param\n",
    "        r2 (float): additional social influence param\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The best clustering found through the algorithm\n",
    "    \"\"\"\n",
    "    # Define initial population (1.)\n",
    "    population = init_population(pop_size, K, data_dim)\n",
    "    # Create a copy of the population to store the best versions found so far for each particle\n",
    "    local_best = population.copy()\n",
    "    # Store the local best fitnesses to avoid recomputing\n",
    "    local_best_f = np.zeros([pop_size])\n",
    "    # Zero-initialize the velocities of each particle\n",
    "    velocities = np.zeros([pop_size, K, data_dim])\n",
    "    # Run the iterations (2.)\n",
    "    for i in range(max_iter):\n",
    "        # Loop over each particle \n",
    "        for idx, particle in enumerate(population):\n",
    "            # Collect the assignments of each datapoint for the particle\n",
    "            assignments = []\n",
    "            # Loop over each datapoint\n",
    "            for z in dataset:\n",
    "                # Collect the distance to each centroid\n",
    "                distances = []\n",
    "                # Calculate the distance for every cluster centroid\n",
    "                for cluster in range(K):\n",
    "                    centroid = particle[:,cluster]\n",
    "                    # Get the distance between the centroid and the datapoint\n",
    "                    distance = calc_distance(centroid, z)\n",
    "                    # Add the distance to the collector\n",
    "                    distances.append(distance)\n",
    "                # The datapoint is assigned to the cluster that minimizes the distance between centroid and datapoint\n",
    "                assignments.append(np.argmin(distances))\n",
    "            # Calculate the fitness of the particle given the new assignments\n",
    "            fitness = calc_fitness(assignments, dataset, particle)\n",
    "            # Update the local best if the new fitness is higher than that of the previous local best\n",
    "            if fitness < local_best_f[idx]:\n",
    "                local_best[idx] = particle.copy()\n",
    "                local_best_f = fitness\n",
    "        # Update the global best to be the best of the local_best\n",
    "        best_idx = np.argmin(local_best_f)\n",
    "        global_best = local_best[best_idx].copy()\n",
    "        # Update the position and velocity of each particle\n",
    "        for i in range(pop_size):\n",
    "            velocities[i] = update_velocity(velocities[i], local_best[i], global_best, population[i], omega, alpha1, alpha2, r1, r2)\n",
    "            population[i] = update_position(population[i], velocities[i])\n",
    "    return global_best   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Implement the k-means clustering method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Generate Artificial dataset 1 using the description given in the above mentioned\n",
    "paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2741cac713f42d9b83073bbc7ba0c967fcf3d886dedc3c40b64d8f3c8efd59a9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
